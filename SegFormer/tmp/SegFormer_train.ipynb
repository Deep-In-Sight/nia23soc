{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda \n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor -- Use SegformerFeatureExtractor\n",
    "* `SegformerFeatureExtractor` will be deprecated soon, please use `SegformerImageProcessor` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from utils import poly2mask, discrete_cmap, label2id, id2label\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"nia23soc_local/1.원천데이터/지하시설물/도로터널/\"\n",
    "label_dir = \"nia23soc_local/2.라벨링데이터/지하시설물/도로터널/\"\n",
    "label_list = glob(label_dir+\"/*.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mask maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels = []\n",
    "for ll in label_list:\n",
    "    poly2mask(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = glob(\"./nia23soc_local/mask/*.png\")\n",
    "\n",
    "#for ml in mask_list:\n",
    "#    img = cv2.imread(ml, cv2.IMREAD_UNCHANGED)\n",
    "    #print(img.max(), id2label[img.max()])\n",
    "#plt.imshow(img, cmap=\"gray\")\n",
    "#plt.suptitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cast_dataset(image, label):\n",
    "    dataset = Dataset.from_dict({\"pixel_values\": sorted(image),\n",
    "                                 \"label\": sorted(label)})\n",
    "    dataset = dataset.cast_column(\"pixel_values\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "def create_dataset(image_paths, label_paths, test_size=0.2, valid_size=0.1, random_state=123):\n",
    "    img_train, img_rem, label_train, label_rem = train_test_split(image_paths, \n",
    "                                                                    label_paths, \n",
    "                                                                    test_size=test_size+valid_size, \n",
    "                                                                    random_state=random_state)\n",
    "    \n",
    "    img_valid, img_test, label_valid, label_test = train_test_split(img_rem, \n",
    "                                                                    label_rem, \n",
    "                                                                    test_size = 1 - valid_size/test_size, \n",
    "                                                                    random_state=random_state)\n",
    "    \n",
    "    train_dataset = cast_dataset(img_train, label_train)\n",
    "    test_dataset = cast_dataset(img_test, label_test)\n",
    "    valid_dataset = cast_dataset(img_valid, label_valid)\n",
    "\n",
    "    return train_dataset, test_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_list = glob(img_dir+\"/*.jpg\")\n",
    "# step 1: create Dataset objects\n",
    "train_dataset, test_dataset, validation_dataset = create_dataset(img_list, mask_list, test_size=0.2, valid_size=0.1, random_state=123)\n",
    "#validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n",
    "\n",
    "# step 2: create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset, # optional??\n",
    "    \"validation\": validation_dataset,\n",
    "  }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segformer expects 'id2label.json' file\n",
    "with open('id2label.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import SegformerFeatureExtractor, SegformerImageProcessor\n",
    "\n",
    "#feature_extractor = SegformerFeatureExtractor()\n",
    "feature_extractor = SegformerImageProcessor(do_reduce_labels=True)\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1) \n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [x for x in example_batch['label']]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.classifier.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.3.proj.bias', 'decode_head.batch_norm.bias', 'decode_head.linear_c.3.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.00006\n",
    "batch_size = 8\n",
    "\n",
    "#hub_model_id = \"segformer-b0-finetuned-segments-sidewalk-2\"\n",
    "\n",
    "# Refer to https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments \n",
    "# for more details on the Trainer arguments\n",
    "training_args = TrainingArguments(\n",
    "    \"segformer-b0-finetuned-segments-sidewalk-outputs\",\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    dataloader_num_workers=6,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    resume_from_checkpoint=True,\n",
    "    #hub_model_id=hub_model_id,\n",
    "    #hub_strategy=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metric (mIOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    print(\"LABELS\", np.unique(labels))\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    print(\"PRED LABELS\", np.unique(pred_labels))\n",
    "    # currently using _compute instead of compute\n",
    "    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "    metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=0,\n",
    "            reduce_labels=feature_extractor.do_reduce_labels,\n",
    "        )\n",
    "    \n",
    "    # add per category metrics as individual key-value pairs\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i+1]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i+1]}\": v for i, v in enumerate(per_category_iou)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "train_dataset.set_transform(train_transforms)\n",
    "test_dataset.set_transform(val_transforms)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoseung\u001b[0m (\u001b[33mdinsight\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hoseung/NIA23/wandb/run-20230913_140446-ibj4rcqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dinsight/huggingface/runs/ibj4rcqi' target=\"_blank\">lucky-fire-6</a></strong> to <a href='https://wandb.ai/dinsight/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dinsight/huggingface' target=\"_blank\">https://wandb.ai/dinsight/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dinsight/huggingface/runs/ibj4rcqi' target=\"_blank\">https://wandb.ai/dinsight/huggingface/runs/ibj4rcqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d37cb5790504fdb870c3ae67b47c222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3334, 'learning_rate': 5.999646017699116e-05, 'epoch': 0.0}\n",
      "{'loss': 2.2355, 'learning_rate': 5.99929203539823e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2289, 'learning_rate': 5.9989380530973456e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2742, 'learning_rate': 5.99858407079646e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5313, 'learning_rate': 5.9982300884955754e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4168, 'learning_rate': 5.997876106194691e-05, 'epoch': 0.02}\n",
      "{'loss': 2.3657, 'learning_rate': 5.997522123893805e-05, 'epoch': 0.02}\n",
      "{'loss': 2.1719, 'learning_rate': 5.997168141592921e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2563, 'learning_rate': 5.996814159292035e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2219, 'learning_rate': 5.996460176991151e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2011, 'learning_rate': 5.9961061946902656e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2442, 'learning_rate': 5.9957522123893805e-05, 'epoch': 0.04}\n",
      "{'loss': 2.1995, 'learning_rate': 5.995398230088496e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2079, 'learning_rate': 5.99504424778761e-05, 'epoch': 0.04}\n",
      "{'loss': 2.0486, 'learning_rate': 5.994690265486726e-05, 'epoch': 0.04}\n",
      "{'loss': 2.1933, 'learning_rate': 5.994336283185841e-05, 'epoch': 0.05}\n",
      "{'loss': 2.1764, 'learning_rate': 5.993982300884956e-05, 'epoch': 0.05}\n",
      "{'loss': 2.1738, 'learning_rate': 5.9936283185840713e-05, 'epoch': 0.05}\n",
      "{'loss': 2.059, 'learning_rate': 5.9932743362831856e-05, 'epoch': 0.06}\n",
      "{'loss': 1.959, 'learning_rate': 5.992920353982301e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297ba799f2f94be8842699e12d1bd852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELS [  0   1   2   3   6 255]\n",
      "PRED LABELS [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1880223751068115, 'eval_mean_iou': 4.772636202419976e-05, 'eval_mean_accuracy': 0.07097405266687253, 'eval_overall_accuracy': 0.025371359403701384, 'eval_accuracy_crack': nan, 'eval_accuracy_reticular crack': 0.1017757069546733, 'eval_accuracy_detachment': 3.533993483316017e-05, 'eval_accuracy_spalling': nan, 'eval_accuracy_efflorescence': nan, 'eval_accuracy_leak': 0.1111111111111111, 'eval_accuracy_rebar': nan, 'eval_accuracy_material separation': nan, 'eval_accuracy_exhilartion': nan, 'eval_accuracy_damage': nan, 'eval_iou_crack': 0.0, 'eval_iou_reticular crack': 0.00047463867379946457, 'eval_iou_detachment': 1.2836265221885114e-06, 'eval_iou_spalling': 0.0, 'eval_iou_efflorescence': 0.0, 'eval_iou_leak': 1.341319920344533e-06, 'eval_iou_rebar': 0.0, 'eval_iou_material separation': 0.0, 'eval_iou_exhilartion': 0.0, 'eval_iou_damage': 0.0, 'eval_runtime': 15.9854, 'eval_samples_per_second': 36.408, 'eval_steps_per_second': 4.567, 'epoch': 0.06}\n",
      "{'loss': 2.2073, 'learning_rate': 5.992566371681416e-05, 'epoch': 0.06}\n",
      "{'loss': 2.1221, 'learning_rate': 5.992212389380531e-05, 'epoch': 0.06}\n",
      "{'loss': 2.0642, 'learning_rate': 5.9918584070796466e-05, 'epoch': 0.07}\n",
      "{'loss': 2.1982, 'learning_rate': 5.991504424778761e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0974, 'learning_rate': 5.9911504424778764e-05, 'epoch': 0.07}\n",
      "{'loss': 2.1201, 'learning_rate': 5.9907964601769913e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0711, 'learning_rate': 5.990442477876106e-05, 'epoch': 0.08}\n",
      "{'loss': 2.367, 'learning_rate': 5.990088495575221e-05, 'epoch': 0.08}\n",
      "{'loss': 2.0769, 'learning_rate': 5.989734513274336e-05, 'epoch': 0.09}\n",
      "{'loss': 2.2233, 'learning_rate': 5.989380530973452e-05, 'epoch': 0.09}\n",
      "{'loss': 2.0887, 'learning_rate': 5.9890265486725666e-05, 'epoch': 0.09}\n",
      "{'loss': 2.0032, 'learning_rate': 5.9886725663716815e-05, 'epoch': 0.09}\n",
      "{'loss': 2.1663, 'learning_rate': 5.9883185840707964e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8894, 'learning_rate': 5.9879646017699114e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9522, 'learning_rate': 5.987610619469027e-05, 'epoch': 0.1}\n",
      "{'loss': 1.9886, 'learning_rate': 5.987256637168142e-05, 'epoch': 0.11}\n",
      "{'loss': 2.0168, 'learning_rate': 5.986902654867257e-05, 'epoch': 0.11}\n",
      "{'loss': 2.2575, 'learning_rate': 5.986548672566372e-05, 'epoch': 0.11}\n",
      "{'loss': 2.042, 'learning_rate': 5.9861946902654866e-05, 'epoch': 0.12}\n",
      "{'loss': 2.293, 'learning_rate': 5.9858407079646015e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8753ad056ccb432ab07ce395aee0bb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hoseung/NIA23/example_SegFormer_evaluate.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hoseung/NIA23/example_SegFormer_evaluate.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1554\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1555\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1556\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1557\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1558\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer.py:1927\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1925\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1927\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer.py:2254\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2254\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2257\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer.py:2968\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2965\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2967\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2968\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2969\u001b[0m     eval_dataloader,\n\u001b[1;32m   2970\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2971\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2972\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2973\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2974\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2975\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2976\u001b[0m )\n\u001b[1;32m   2978\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer.py:3202\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3200\u001b[0m \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3201\u001b[0m     logits \u001b[39m=\u001b[39m nested_numpify(preds_host)\n\u001b[0;32m-> 3202\u001b[0m     all_preds \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(all_preds, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3203\u001b[0m \u001b[39mif\u001b[39;00m inputs_host \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3204\u001b[0m     inputs_decode \u001b[39m=\u001b[39m nested_numpify(inputs_host)\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:122\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[1;32m    119\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[1;32m    123\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported type for concatenation: got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nia23/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:93\u001b[0m, in \u001b[0;36mnumpy_pad_and_concatenate\u001b[0;34m(array1, array2, padding_index)\u001b[0m\n\u001b[1;32m     90\u001b[0m array2 \u001b[39m=\u001b[39m atleast_1d(array2)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(array1\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mconcatenate((array1, array2), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     95\u001b[0m \u001b[39m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     96\u001b[0m new_shape \u001b[39m=\u001b[39m (array1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m array2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(array1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], array2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m array1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuend_segformer_b0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
