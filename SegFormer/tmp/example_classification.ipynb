{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a7da1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6644767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/anaconda3/envs/hug/lib/python3.11/site-packages/datasets/load.py:2080: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=all_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset('cifar10',\n",
    "                            split='train',\n",
    "                            ignore_verifications=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb323da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/anaconda3/envs/hug/lib/python3.11/site-packages/datasets/load.py:2080: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=all_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_test = load_dataset('cifar10',\n",
    "                            split='test',\n",
    "                            ignore_verifications=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505bd5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c253268b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec88b60",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb9f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5854423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/anaconda3/envs/hug/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ce97fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228aec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = feature_extractor(\n",
    "            dataset_train[0]['img'],\n",
    "            return_tensors='pt') # pt = Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e14423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
      "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
      "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
      "          ...,\n",
      "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
      "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
      "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863]],\n",
      "\n",
      "         [[ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
      "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
      "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
      "          ...,\n",
      "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
      "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
      "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412]],\n",
      "\n",
      "         [[ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          ...,\n",
      "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
      "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
      "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961]]]])}\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488dfab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(example['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e4cab",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8614b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/anaconda3/envs/hug/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2416ec",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Requirements \n",
    "1. training and test dataset\n",
    "2. feature extractor\n",
    "3. model\n",
    "4. collate function // data input 모양 맞추기 \n",
    "5. evaluation metric\n",
    "6. ... other training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "841099c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collated_fn(batch):\n",
    "    return {'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['label'] for x in batch])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63864826",
   "metadata": {},
   "source": [
    "#### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7322c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate \n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "    predictions = np.argmax(p.predicts, axis=1),\n",
    "    references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be493e",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc3e606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cifar\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ad6fc",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "880f7ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = dataset_train.features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370f840",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a189d154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9c72d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efa610d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385b7853728f4146ae371b38d03adaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1dc00079b74f3182222bb51fa35074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dacaadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb0be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    images = [_transforms(img.convert(\"RGB\")) for img in examples[\"img\"]]\n",
    "    examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e976fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.set_transform(transforms)\n",
    "dataset_test.set_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab94a7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['img', 'label', 'pixel_values'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc510fc",
   "metadata": {},
   "source": [
    "Now the dataset has \"pixel_values\" key required by the VIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00d004b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc9184a1c10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = dataset_train[10][\"pixel_values\"]\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a9cc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=collated_fn,\n",
    "    compute_metrics=compute_metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e418378",
   "metadata": {},
   "source": [
    "#### Run trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5251b2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e39deeaa0d47d4b1d2bbd3b2d2df31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m      3\u001b[0m trainer\u001b[39m.\u001b[39mlog_metrics(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, train_results\u001b[39m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1554\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1555\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1556\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1557\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1558\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/transformers/trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1813\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1814\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1815\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/hug/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m, in \u001b[0;36mcollated_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollated_fn\u001b[39m(batch):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mstack([x[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]),\n\u001b[1;32m      3\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch])}\n",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollated_fn\u001b[39m(batch):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mstack([x[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]),\n\u001b[1;32m      3\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor([x[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch])}\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pixel_values'"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca3c1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset_train['img'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e848927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7667141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
